{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re #regular expressions\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analyse\n",
    "\n",
    "Bij sentiment analyse gaan we op een geautomatiseerde manier na welk sentiment een bepaald bericht oproept. Deze berichten kunnen bijvoorbeeld afkomstig zijn van posts op sociale media, reviews, opinies en dergelijke. \n",
    "\n",
    "Bij deze opdracht is het de bedoeling om tekstberichten op te delen in twee sentiment categorieën, namelijk positief of negatief.\n",
    "De trainingset is te vinden via het bestand *sentiment_train.csv* en de testset via het bestand *sentiment_test.csv*.\n",
    "\n",
    "Doorloop daarbij de volgende stappen:\n",
    "\n",
    "- Vooranalyse van de data: hoe is de dataset verdeeld? Zijn er evenveel positieve als negatieve sentimenten? Is er bijvoorbeeld een verband tussen de lengte van de tekst en het sentiment? \n",
    "-  Preprocessing van de tekst: opkuisen van de tekst (stopwoorden, niet-letters verwijderen, omzetten naar lowercase, ...)\n",
    "- Toepassen van stemming.\n",
    "- Omzetten naar een bag of words (test hierbij verschillende modellen) en gebruik mogelijks n-grams\n",
    "- train verschillende classifiers: naive Bayes, logistic regression en SVM. \n",
    "- Test de performantie van de verschillende getrainde classifiers via de testset. Kijk daarbij naar de accuracy en f1-score. Interpreteer de resultaten. Kijk ook naar welk algoritme het snelste traint. Bekijk de teksten die verkeerd geclassificeerd werden en zoek naar de oorzaak.\n",
    "\n",
    "Test het model ook eens met een eigen korte dataset aan sentiment bevattende zinnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5918, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentiment_train = pd.read_csv('sentiment_train.csv')\n",
    "data_sentiment_test = pd.read_csv('sentiment_test.csv')\n",
    "\n",
    "data_sentiment_test.head()\n",
    "data_sentiment_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    2535\n",
       "1    3383\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vooranalyse\n",
    "\n",
    "\n",
    "#aantal klasses:\n",
    "\n",
    "aantal = data_sentiment_train.sentiment.unique()\n",
    "print(aantal)\n",
    "\n",
    "#gebalanceerd?\n",
    "\n",
    "g = data_sentiment_train.groupby('sentiment')\n",
    "g.sentiment.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redelijk gebalanceerd (niet volledig maar veel samples dus ik denk wel dat het oké is :-))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment        \n",
       "0          lengte    66.005917\n",
       "           lengte    66.005917\n",
       "           lengte    66.005917\n",
       "1          lengte    58.606562\n",
       "           lengte    58.606562\n",
       "           lengte    58.606562\n",
       "Name: lengte, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verband lengte en het sentiment?\n",
    "data_sentiment_train.insert(0, \"lengte\", data_sentiment_train['text'].apply(len) ,allow_duplicates=True)\n",
    "\n",
    "g = data_sentiment_train.groupby('sentiment')\n",
    "g.lengte.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#de berichtjes met een negatief sentiment zijn een beetje langer dan de berichten met een positief sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opsplitsen in features & values\n",
    "X_train = data_sentiment_train.text.values\n",
    "y_train = data_sentiment_train.sentiment.values\n",
    "\n",
    "X_test = data_sentiment_test.text.values \n",
    "y_test = data_sentiment_test.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " #preprocessing (= opkuisen) van de dataset\n",
    "def text_preprocessing(text, language, minWordSize):\n",
    "    \n",
    "    # remove html\n",
    "    text_no_html = BeautifulSoup(str(text),\"html.parser\" ).get_text()\n",
    "    \n",
    "    # remove non-letters\n",
    "    text_alpha_chars = re.sub(\"[^a-zA-Z']\", \" \", str(text_no_html)) \n",
    "        \n",
    "    # convert to lower-case\n",
    "    text_lower = text_alpha_chars.lower()\n",
    "    \n",
    "    # remove stop words\n",
    "    stops = set(stopwords.words(language)) \n",
    "    text_no_stop_words = ' '\n",
    "    \n",
    "    for w in text_lower.split():\n",
    "        if w not in stops:  \n",
    "            text_no_stop_words = text_no_stop_words + w + ' '\n",
    "      \n",
    "       # do stemming\n",
    "    text_stemmer = ' '\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    for w in text_no_stop_words.split():\n",
    "        text_stemmer = text_stemmer + stemmer.stem(w) + ' '\n",
    "         \n",
    "    # remove short words\n",
    "    text_no_short_words = ' '\n",
    "    for w in text_stemmer.split(): \n",
    "        if len(w) >=minWordSize:\n",
    "            text_no_short_words = text_no_short_words + w + ' '\n",
    " \n",
    "\n",
    "    return text_no_short_words\n",
    "\n",
    "\n",
    "language = 'english'\n",
    "minWordLength = 2\n",
    "\n",
    "for i in range(X_train.size):\n",
    "    X_train[i] = text_preprocessing(X_train[i], language, minWordLength)\n",
    "    \n",
    "    \n",
    "for i in range(X_test.size):\n",
    "    X_test[i] = text_preprocessing(X_test[i], language, minWordLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 88)\t1\n",
      "  (0, 586)\t1\n",
      "  (0, 631)\t1\n",
      "  (0, 993)\t1\n",
      "  (0, 1080)\t1\n",
      "  (0, 1131)\t1\n",
      "  (0, 1238)\t1\n",
      "  (0, 1328)\t1\n",
      "  (0, 1338)\t1\n",
      "  (1, 340)\t1\n",
      "  (1, 631)\t2\n",
      "  (1, 860)\t1\n",
      "  (1, 993)\t1\n",
      "  (1, 1080)\t2\n",
      "  (1, 1098)\t1\n",
      "  (1, 1338)\t1\n",
      "  (2, 455)\t1\n",
      "  (2, 718)\t1\n",
      "  (2, 923)\t1\n",
      "  (3, 247)\t1\n",
      "  (3, 314)\t1\n",
      "  (3, 601)\t1\n",
      "  (3, 970)\t1\n",
      "  (3, 1353)\t1\n",
      "  (3, 1502)\t1\n",
      "  :\t:\n",
      "  (997, 365)\t1\n",
      "  (997, 631)\t1\n",
      "  (997, 1080)\t1\n",
      "  (997, 1207)\t1\n",
      "  (998, 247)\t1\n",
      "  (998, 314)\t1\n",
      "  (998, 798)\t1\n",
      "  (998, 860)\t1\n",
      "  (998, 948)\t1\n",
      "  (998, 1035)\t1\n",
      "  (998, 1502)\t1\n",
      "  (998, 1560)\t1\n",
      "  (999, 115)\t1\n",
      "  (999, 122)\t1\n",
      "  (999, 631)\t1\n",
      "  (999, 634)\t1\n",
      "  (999, 892)\t1\n",
      "  (999, 942)\t1\n",
      "  (999, 948)\t1\n",
      "  (999, 1008)\t1\n",
      "  (999, 1058)\t1\n",
      "  (999, 1080)\t1\n",
      "  (999, 1131)\t1\n",
      "  (999, 1346)\t1\n",
      "  (999, 1405)\t1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#bag of words aanmaken\n",
    "count_vect = CountVectorizer()\n",
    "X_train_bag_of_words = count_vect.fit(X_train)\n",
    "X_train_bag_of_words = count_vect.transform(X_train)\n",
    "X_test_bag_of_words = count_vect.transform(X_test)\n",
    "\n",
    "print(X_test_bag_of_words)\n",
    "\n",
    "#tfidf transformer: dit is feite alles nog eens delen door hoeveel keer het voorkomt (?)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_bag_of_words)\n",
    "X_train_tf = tf_transformer.transform(X_train_bag_of_words)\n",
    "X_test_tf = tf_transformer.transform(X_test_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hoe je dit moet lezen: in rij 0, kwam woord 88 1x voor enz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97       440\n",
      "          1       0.97      0.99      0.98       560\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1000\n",
      "\n",
      "[[422  18]\n",
      " [  6 554]]\n",
      "97.6\n"
     ]
    }
   ],
   "source": [
    "#naive bayes classifier\n",
    "\n",
    "NBclassifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "NBclassifier.fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred = NBclassifier.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we bekomen een accuracy van 97.6%! met een alpha waarde van 0.1 (deze alpha bepaalt hoe sterk dat niet geziene woorden zullen\n",
    "#meetellen (laplacian smoothing) = (hoeveel keer het woord voorkomt + alpha) / (totaal aantal woorden + alpha* #verschillende w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9961135518756337\n",
      "Best parameters : {'C': 1000}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       440\n",
      "          1       0.99      1.00      1.00       560\n",
      "\n",
      "avg / total       1.00      0.99      0.99      1000\n",
      "\n",
      "[[436   4]\n",
      " [  1 559]]\n",
      "99.5\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "paramaters = [\n",
    "             {'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000,10000, 100000]}                                       \n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train_tf, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we bekomen een accuracy van 99.5%! (beter dan naive bayes)\n",
    "#de beste c parameter is 1000, dit is niet sterk geregulariseerd (inverse) en dus maw kans op underfitting\n",
    "#ook heb ik een cross validation van 4 genomen, dit wil zeggen dat hij de training in 4 stukken opdeelt\n",
    "#en op elk van deze 4 eens zal valideren (het duurt dus nog 4 keer zo lang (!))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9962825278810409\n",
      "Best parameters : {'C': 10, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       440\n",
      "          1       0.99      1.00      0.99       560\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1000\n",
      "\n",
      "[[436   4]\n",
      " [  2 558]]\n",
      "99.4\n"
     ]
    }
   ],
   "source": [
    "#proberen met SVM\n",
    "\n",
    "model = svm.SVC()\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], 'C': np.linspace(1,20,100)},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},\n",
    "        {'kernel': ['poly'], 'C':[1, 10]} ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train_tf, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we hebben een accuracy van 99.4% op de testset\n",
    "# de beste kernel is een rbf (radial basis function = gaussiaanse kernel) met een gamme van 0.2.\n",
    "#deze gamma bepaalt de breedte van de kernel, kleine gamma betekent smalle kernel, kans op overfitting\n",
    "#als regularisatieparameter koos de gridsearch voor 10, hoe hoger, hoe sterker geregulariseerd\n",
    "#(hoe groter de margin van de support vectors tov de scheidingslijn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Movie categorization\n",
    "\n",
    "Bij deze opdracht zullen classifiers getraind worden om op basis van de beschrijving van een film **één of meerdere genres** aan de film toe te kennen.\n",
    "Het bestand *Moviedata.csv* bevat van meer dan 40000 films de beschrijving samen met het genre/genres van de film.\n",
    "\n",
    "Splits de data op in een training- en testset zodat er zeker 5000 samples in de testset zitten (bijvoorbeeld de laatste 5000 films van het Moviedata.csv bestand). De resterende films kunnen gebruikt worden om de classifier mee te trainen.\n",
    "\n",
    "Doe een vooranalyse van de data: bekijk of de data gebalanceerd is. Komt elk genre met andere woorden evenveel voor? Hoeveel films zijn er met maar 1 genre, hoeveel met 2, etc.\n",
    "\n",
    "Aangezien een film tot meerdere genres kan behoren is het aangewezen om voor elk genre een aparte classifier te trainen. \n",
    "\n",
    "Gebruik zowel logistic resgression, SVM als Naive Bayes. \n",
    "\n",
    "Bespreek de resultaten op de testset:\n",
    "- Wat zijn de accuracy en de f1-score per genre van de verschillende classifiers (logistic regression, SVM en Naive Bayes.\n",
    "- Wat is de accuracy op basis van volledig correcte classificaties van de films in de trainingset? Hierbij kijk je of een bepaalde film correct aan alle genres is toegekend.\n",
    "- Welke classifier geniet jouw voorkeur? Bespreek waarom.\n",
    "\n",
    "Zoek op IMDB.com naar een aantal recente films (die nog niet in de dataset aanwezig zijn) en laat deze classificeren en evalueer de resultaten.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_movie = pd.read_csv('Moviedata.csv',';')\n",
    "#hier drop ik alle waarden die niet 1.0 of 0.0 zijn (anders crasht het trainen van mijn model... )\n",
    "#ik doe het enkel voor de action table omdat ik ook enkel deze zal trainen (anders té veel werk dat op hetzelfde neerkomt)\n",
    "data_movie.drop(data_movie[(data_movie.Action != 1.0) & (data_movie.Action != 0.0) ].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>...</th>\n",
       "      <th>History</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Music</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Science-Fiction</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>TV Movie</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killing Zoe</td>\n",
       "      <td>Zed (Eric Stoltz) is an American vault-cracker...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Arrival</td>\n",
       "      <td>Zane Ziminski is an astrophysicist who receive...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Man in the Iron Mask</td>\n",
       "      <td>Years have passed since the Three Musketeers, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Gods Must Be Crazy II</td>\n",
       "      <td>Xixo is back again. This time, his children ac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Superman IV: The Quest for Peace</td>\n",
       "      <td>With global superpowers engaged in an increasi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     original_title  \\\n",
       "0                       Killing Zoe   \n",
       "1                       The Arrival   \n",
       "2          The Man in the Iron Mask   \n",
       "3         The Gods Must Be Crazy II   \n",
       "4  Superman IV: The Quest for Peace   \n",
       "\n",
       "                                            overview  Action  Adventure  \\\n",
       "0  Zed (Eric Stoltz) is an American vault-cracker...     1.0        0.0   \n",
       "1  Zane Ziminski is an astrophysicist who receive...     1.0        0.0   \n",
       "2  Years have passed since the Three Musketeers, ...     1.0        1.0   \n",
       "3  Xixo is back again. This time, his children ac...     1.0        0.0   \n",
       "4  With global superpowers engaged in an increasi...     1.0        1.0   \n",
       "\n",
       "   Animation  Comedy  Crime  Documentary  Drama  Family   ...     History  \\\n",
       "0        0.0     0.0    1.0          0.0    1.0     0.0   ...         0.0   \n",
       "1        0.0     0.0    0.0          0.0    0.0     0.0   ...         0.0   \n",
       "2        0.0     0.0    0.0          0.0    1.0     0.0   ...         0.0   \n",
       "3        0.0     1.0    0.0          0.0    0.0     0.0   ...         0.0   \n",
       "4        0.0     0.0    0.0          0.0    0.0     0.0   ...         0.0   \n",
       "\n",
       "   Horror  Music  Mystery  Romance  Science-Fiction  Thriller  TV Movie  War  \\\n",
       "0     0.0    0.0      0.0      0.0              0.0       1.0       0.0  0.0   \n",
       "1     0.0    0.0      1.0      0.0              1.0       1.0       0.0  0.0   \n",
       "2     0.0    0.0      0.0      0.0              0.0       0.0       0.0  0.0   \n",
       "3     0.0    0.0      0.0      0.0              0.0       0.0       0.0  0.0   \n",
       "4     0.0    0.0      0.0      0.0              1.0       0.0       0.0  0.0   \n",
       "\n",
       "   Western  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voor genre action (ik zal het nu niet doen voor alle genres, lijkt me wat bandwerk)\n",
    "\n",
    "#opsplitsen in features & values\n",
    "X = data_movie.overview.values\n",
    "y = data_movie.Action.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opsplitsen in train & testset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanbe\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "#preprocessing van de tekst\n",
    "def text_preprocessing(text, language, minWordSize):\n",
    "    \n",
    "    # remove html\n",
    "    text_no_html = BeautifulSoup(str(text),\"html.parser\" ).get_text()\n",
    "    \n",
    "    # remove non-letters\n",
    "    text_alpha_chars = re.sub(\"[^a-zA-Z']\", \" \", str(text_no_html)) \n",
    "        \n",
    "    # convert to lower-case\n",
    "    text_lower = text_alpha_chars.lower()\n",
    "    \n",
    "    # remove stop words\n",
    "    stops = set(stopwords.words(language)) \n",
    "    text_no_stop_words = ' '\n",
    "    \n",
    "    for w in text_lower.split():\n",
    "        if w not in stops:  \n",
    "            text_no_stop_words = text_no_stop_words + w + ' '\n",
    "      \n",
    "       # do stemming\n",
    "    text_stemmer = ' '\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    for w in text_no_stop_words.split():\n",
    "        text_stemmer = text_stemmer + stemmer.stem(w) + ' '\n",
    "         \n",
    "    # remove short words\n",
    "    text_no_short_words = ' '\n",
    "    for w in text_stemmer.split(): \n",
    "        if len(w) >=minWordSize:\n",
    "            text_no_short_words = text_no_short_words + w + ' '\n",
    " \n",
    "\n",
    "    return text_no_short_words\n",
    "\n",
    "\n",
    "language = 'english'\n",
    "minWordLength = 2\n",
    "\n",
    "for i in range(X_train.size):\n",
    "    X_train[i] = text_preprocessing(X_train[i], language, minWordLength)\n",
    "    \n",
    "    \n",
    "for i in range(X_test.size):\n",
    "    X_test[i] = text_preprocessing(X_test[i], language, minWordLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1226)\t1\n",
      "  (0, 2145)\t1\n",
      "  (0, 3081)\t1\n",
      "  (0, 3174)\t1\n",
      "  (0, 5435)\t1\n",
      "  (0, 6290)\t1\n",
      "  (0, 6373)\t1\n",
      "  (0, 7629)\t1\n",
      "  (0, 7874)\t1\n",
      "  (0, 8620)\t1\n",
      "  (0, 8668)\t1\n",
      "  (0, 9650)\t1\n",
      "  (0, 10844)\t1\n",
      "  (0, 11663)\t1\n",
      "  (0, 12722)\t1\n",
      "  (0, 13489)\t1\n",
      "  (0, 13600)\t1\n",
      "  (0, 13647)\t1\n",
      "  (0, 14010)\t1\n",
      "  (0, 14301)\t1\n",
      "  (0, 14619)\t1\n",
      "  (0, 14992)\t1\n",
      "  (0, 15191)\t1\n",
      "  (0, 15352)\t2\n",
      "  (0, 15578)\t1\n",
      "  :\t:\n",
      "  (8456, 45294)\t1\n",
      "  (8456, 45805)\t1\n",
      "  (8456, 45851)\t2\n",
      "  (8457, 4602)\t1\n",
      "  (8457, 5577)\t1\n",
      "  (8457, 6429)\t1\n",
      "  (8457, 8357)\t1\n",
      "  (8457, 8704)\t1\n",
      "  (8457, 10095)\t1\n",
      "  (8457, 12537)\t1\n",
      "  (8457, 15194)\t1\n",
      "  (8457, 18403)\t1\n",
      "  (8457, 18835)\t1\n",
      "  (8457, 19381)\t2\n",
      "  (8457, 26080)\t1\n",
      "  (8457, 26477)\t1\n",
      "  (8457, 30433)\t1\n",
      "  (8457, 35654)\t1\n",
      "  (8457, 37688)\t1\n",
      "  (8457, 38178)\t1\n",
      "  (8457, 38313)\t1\n",
      "  (8457, 42044)\t1\n",
      "  (8457, 44595)\t2\n",
      "  (8457, 44727)\t1\n",
      "  (8457, 45477)\t1\n"
     ]
    }
   ],
   "source": [
    "#aanmaken van de bag of words + tfidf\n",
    "count_vect = CountVectorizer()\n",
    "X_train_bag_of_words = count_vect.fit(X_train)\n",
    "X_train_bag_of_words = count_vect.transform(X_train)\n",
    "X_test_bag_of_words = count_vect.transform(X_test)\n",
    "\n",
    "print(X_test_bag_of_words)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_bag_of_words)\n",
    "X_train_tf = tf_transformer.transform(X_train_bag_of_words)\n",
    "X_test_tf = tf_transformer.transform(X_test_bag_of_words)µ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.98      0.93      7164\n",
      "        1.0       0.75      0.26      0.39      1294\n",
      "\n",
      "avg / total       0.86      0.87      0.85      8458\n",
      "\n",
      "[[7050  114]\n",
      " [ 955  339]]\n",
      "87.36107826909435\n"
     ]
    }
   ],
   "source": [
    "#modellen trainen\n",
    "\n",
    "#naive bayes\n",
    "\n",
    "NBclassifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "NBclassifier.fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred = NBclassifier.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ik bekom een accuracy van 87% dit vindt ik niet zo heel veel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.8738176873965476\n",
      "Best parameters : {'C': 10}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.96      0.93      7164\n",
      "        1.0       0.65      0.44      0.53      1294\n",
      "\n",
      "avg / total       0.87      0.88      0.87      8458\n",
      "\n",
      "[[6851  313]\n",
      " [ 719  575]]\n",
      "87.79853393237171\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "paramaters = [\n",
    "             {'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000,10000, 100000]}                                       \n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train_tf, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ik bekom ook hier een accuracy van 87% (net ietsje meer dan naive bayes wel)\n",
    "#de beste C parameter is een 10, dit is de regularisatie, hoe hoger hoe minder geregulariseerd (inverse)\n",
    "#cross validation is 4, wat wil zeggen dat hij alles 4 keer op een (verschillende) validatieset heeft gevalideerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.8772463939465595\n",
      "Best parameters : {'C': 1.1919191919191918, 'kernel': 'linear'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.97      0.93      7164\n",
      "        1.0       0.70      0.41      0.51      1294\n",
      "\n",
      "avg / total       0.87      0.88      0.87      8458\n",
      "\n",
      "[[6936  228]\n",
      " [ 769  525]]\n",
      "88.21234334358005\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "model = svm.SVC()\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], 'C': np.linspace(1,20,100)},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},\n",
    "        {'kernel': ['poly'], 'C':[1, 10]} ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train_tf, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ik bekom een accuracy van 88%! (duurde wel heel lang om het te trainen... )\n",
    "#Beste parameters zijn een C van 1.9, dit is niet sterk geregulariseerd (de margin tss scheding & support vectors zal dus\n",
    "#relatief groot zijn.)\n",
    "#we gebruiken een lineaire kernel (dit is eigenlijk geen kernel) en we blijven dus met onze data in 2D werken.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moest ik het voor alle genres willen doen:\n",
    "# een fucnctie maken waar ik met een foreach alle genres overloop,\n",
    "# de data splits en een model aanmaak voor dat genre, plus het trainen met de data\n",
    "#daarna kan ik dit model simpelweg returnen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
