{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.datasets import load_digits\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.io import imread, imshow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht 1 - wine classification en cancer detection via SVM\n",
    "\n",
    "Doe de wine classification en de cancer detection uit vorige opdracht over met support vector machines. Gebruik hiervoor cross-validation pipeline om de beste parameters te zoeken: lineaire kernel vs polynomial of RBF kernel, beste waarde voor C en Gamma. Vergelijk de resultaten met deze die bekomen werden via logistic regression. Welke presteert het best in termen van accuracy, recall, precision en f1 score? Zoek een verklaring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs Alcohol</th>\n",
       "      <th>MalicAcid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>AlcalinityOfAsh</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>TotalPhenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>NonflavanoidsPhenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>ColorIntensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.30</td>\n",
       "      <td>23.6</td>\n",
       "      <td>70</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.07</td>\n",
       "      <td>3.21</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.51</td>\n",
       "      <td>24.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.72</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.37</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.92</td>\n",
       "      <td>19.6</td>\n",
       "      <td>78</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.68</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.48</td>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.11</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.70</td>\n",
       "      <td>15.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>5.30</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.18</td>\n",
       "      <td>502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.38</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.57</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   inputs Alcohol  MalicAcid   Ash  AlcalinityOfAsh  Magnesium  TotalPhenols  \\\n",
       "0           12.08       1.33  2.30             23.6         70          2.20   \n",
       "1           12.08       1.13  2.51             24.0         78          2.00   \n",
       "2           12.37       1.17  1.92             19.6         78          2.11   \n",
       "3           13.11       1.01  1.70             15.0         78          2.98   \n",
       "4           12.04       4.30  2.38             22.0         80          2.10   \n",
       "\n",
       "   flavanoids  NonflavanoidsPhenols  Proanthocyanins  ColorIntensity   Hue  \\\n",
       "0        1.59                  0.42             1.38            1.74  1.07   \n",
       "1        1.58                  0.40             1.40            2.20  1.31   \n",
       "2        2.00                  0.27             1.04            4.68  1.12   \n",
       "3        3.18                  0.26             2.28            5.30  1.12   \n",
       "4        1.75                  0.42             1.35            2.60  0.79   \n",
       "\n",
       "   OD280/OD315  Proline  Cultivar  \n",
       "0         3.21      625         1  \n",
       "1         2.72      630         1  \n",
       "2         3.48      510         1  \n",
       "3         3.18      502         1  \n",
       "4         2.57      580         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wine - SVM\n",
    "df = pd.read_csv('wine_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cultivar\n",
       "0    59\n",
       "1    71\n",
       "2    48\n",
       "Name: Cultivar, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aantal klasses: \n",
    "aantal = df.Cultivar.unique()\n",
    "print(aantal)\n",
    "\n",
    "#gebalanceerd? \n",
    "\n",
    "g = df.groupby('Cultivar')\n",
    "g.Cultivar.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opsplitsen in features & targets\n",
    "features = list(df.columns[:df.columns.size-1])\n",
    "X = df[features].values \n",
    "y= df['Cultivar'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.13720713 -0.90336179 -0.24314178 ...  0.49379744  0.84508517\n",
      "  -0.38816832]\n",
      " [-1.13720713 -1.08289442  0.52447981 ...  1.54675873  0.15298819\n",
      "  -0.37224585]\n",
      " [-0.77898029 -1.0469879  -1.63217132 ...  0.71316437  1.22644473\n",
      "  -0.7543851 ]\n",
      " ...\n",
      " [-0.01311602 -0.59815632  0.8534605  ...  1.54675873  1.25469358\n",
      "   0.75824943]\n",
      " [-0.97662269 -1.02903463 -2.25357928 ...  1.41513857  0.64734317\n",
      "  -0.0920104 ]\n",
      " [-0.6554538  -0.7328058  -0.60867587 ...  0.88865792  0.02586833\n",
      "   0.60539373]]\n"
     ]
    }
   ],
   "source": [
    "#normaliseren van de dataset\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opsplitsen in training-  & test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9788732394366197\n",
      "Best parameters : {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.88      1.00      0.93         7\n",
      "          2       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       0.98      0.97      0.97        36\n",
      "\n",
      "[[12  1  0]\n",
      " [ 0  7  0]\n",
      " [ 0  0 16]]\n",
      "97.22222222222221\n"
     ]
    }
   ],
   "source": [
    "#aanmaken model + gridsearch\n",
    "model = svm.SVC()\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], 'C': np.linspace(1,20,100)},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},\n",
    "        {'kernel': ['poly'], 'C':[1, 10]} ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv stelt het aantal validation sets voor: ik koos voor 4, dit wil zeggen dat hij alles 4 keer zal uitvoeren \n",
    "#voor de wijndata bekom ik met SVM een kleinere accuracy dan met logistic regression. \n",
    "#(maar in feite is dat maar één wijn meer die hij verkeerd geclassificeerd heeft)\n",
    "#adhv grid_search heb ik gevonden dat een c=1, gamma= 0.01 & RBF kernel de beste accurac opleveren. \n",
    "#c1 = weinig regularistatie, dit betekent ook een kleinere margin voor de Support vectors, dus wel kans op underfitting \n",
    "#(maar hier duidelijk niet het geval)\n",
    "#we hebben een kleine gamma, dit beïnvloed de breedte van de kernel, kleine gamme betekent bredere kernel\n",
    "#dit kan leiden tot underfitting (maar ook hier niet het geval)\n",
    "#ik denk dat ik nog niet goed weet wanneer een regularisatie parameter effectief klein is of niet (bv zoveel onder 0 of zoveel boven)\n",
    "#RBF kernel is het gekozen model: dit is een gaussiananse kernel (radial basis function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancer - SVM\n",
    "df = pd.read_csv('cancer.csv')\n",
    "df.diagnosis = df.diagnosis.replace('M',1)\n",
    "df.diagnosis = df.diagnosis.replace('B',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis\n",
       "0    357\n",
       "1    211\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aantal klasses + gebalanceerd? \n",
    "\n",
    "aantal = df.diagnosis.unique()\n",
    "\n",
    "g = df.groupby('diagnosis')\n",
    "g.diagnosis.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min of meer gebalanceerd (er is genoeg data om het verschil te compenseren naar mijn mening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opsplitsen in features & targets\n",
    "features = list(df.columns[2:df.columns.size-1])\n",
    "X = df[features].values \n",
    "y= df['diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.83208378 -0.35832707  1.68910604 ...  1.09526221 -0.24044363\n",
      "   0.28529387]\n",
      " [ 1.58210603  0.45385958  1.56958968 ...  1.96646779  1.16385786\n",
      "   0.2053006 ]\n",
      " [-0.76711673  0.25081292 -0.59077158 ...  2.18809026  6.08623563\n",
      "   4.95045676]\n",
      " ...\n",
      " [ 0.70434323  2.04789258  0.6752776  ...  0.41969578 -1.10613122\n",
      "  -0.31576665]\n",
      " [ 1.84060575  2.33962629  1.98583632 ...  2.30272257  1.93516598\n",
      "   2.22846365]\n",
      " [-1.80679694  1.22170271 -1.81313547 ... -1.74761915 -0.04354852\n",
      "  -0.74961903]]\n"
     ]
    }
   ],
   "source": [
    "#normaliseren van de dataset (dit is uiterst noodzakelijk bij een rbf kernel)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9823788546255506\n",
      "Best parameters : {'C': 1.0, 'kernel': 'linear'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97        71\n",
      "          1       0.98      0.93      0.95        43\n",
      "\n",
      "avg / total       0.97      0.96      0.96       114\n",
      "\n",
      "[[70  1]\n",
      " [ 3 40]]\n",
      "96.49122807017544\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC()\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], 'C': np.linspace(1,20,100)},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},\n",
    "        {'kernel': ['poly'], 'C':[1, 10]} ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ik bekom een slechter resultaat dan met logistic regression\n",
    "#ik koos bij crossvalidation voor 4, dit wil zeggen dat hij alles 4 x zal uitvoeren (want moet op elk stuk eens valideren)\n",
    "#koos voor een regularistatie parameter van 1: dit is weinig regularisatie -> grotere margin -> kans op underfitting\n",
    "#dit keer is een lineaire kernel het beste, dit is infeite geen kernel gebruiken en dus in 2D blijven werken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht 2 - MNIST\n",
    "De MNIST (\"Modified National Institute of Standards and Technology\")is een veelgebruikte dataset voor het testen en benchmarken van klassificatie algoritmes. Het bevat tienduizenden afbeeldingen van handgeschreven getallen. \n",
    "Meer info over deze dataset is te vinden op: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "Bij deze opdracht worden 3 databestanden meegeleverd:\n",
    "- 'train.csv' bevat gelabelde data voor **trainen** van  de classifier.\n",
    "- 'test.csv' bevat gelabelde data voor het **testen** van de getrainde classifier\n",
    "- 'test_Kaggle.csv' bevat niet-gelabelde data voor het evalueren van de classifier via de competitie te vinden op https://www.kaggle.com/c/digit-recognizer. Deze data is dus enkel bruikbaar bij deelname aan de Kaggle competitie. \n",
    "\n",
    "\n",
    "1) Train zowel een logistic regression classifier als een Support Vector Machine (met of zonder kernel. Ga via tuning van de hyperparameters op zoek naar de meest performante classifier. Doe dit door de accuracy te maximaliseren of de error rate te minimaliseren op de test.csv dataset. De error rate = 1- accuracy. Het trainen van Support Vector Machines (zeker deze met kernel) vragen enorm veel rekenkracht. Het is daarom verstandig om in eerste instantie te trainen op een klein deel van de training set. Het trainen via logistic regression is minder belastend voor de CPU, desalniettemin wordt aangeraden om de lbfgs solver te gebruiken. *(LogisticRegression(multi_class='multinomial', solver='lbfgs'))*\n",
    "\n",
    "2) Verzorg de code telkens van commentaar en schrijf jouw conclusies en besluiten neer.\n",
    "\n",
    "3) Test jouw uiteindelijke classifier met een aantal zelf geschreven getallen. Wat zijn de bevindingen? Waarvan hangt classificatienauwkeurigheid af?\n",
    "\n",
    "4) Optioneel: test op de 'test_Kaggle' dataset en laad de resultaten in het juiste formaat op naar de Kaggle website. Wat is de behaalde score? Vergelijk deze score met de score op http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "Het visualiseren van een digit kan met *'plt.imshow(X_train[n].reshape((28, 28)),cmap = 'gray')'*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inlezen van beide datasets + ze limiteren tot 10K records (anders veeel te veel memory nodig)\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train = df_train.loc[1:10000]\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.loc[1:10000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opsplitsen in features & targets\n",
    "features_train = list(df_train.columns[1:df_train.columns.size])\n",
    "X_train = df_train[features_train].values \n",
    "y_train= df_train['label'].values\n",
    "\n",
    "features_test = list(df_test.columns[1:df_test.columns.size])\n",
    "X_test = df_test[features_test].values \n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanbe\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#scalen van de data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9087\n",
      "Best parameters : {'C': 0.01}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95       998\n",
      "          1       0.94      0.98      0.96      1127\n",
      "          2       0.90      0.90      0.90       968\n",
      "          3       0.92      0.89      0.90      1059\n",
      "          4       0.92      0.93      0.93       914\n",
      "          5       0.88      0.86      0.87       914\n",
      "          6       0.94      0.95      0.94       951\n",
      "          7       0.93      0.92      0.92      1066\n",
      "          8       0.90      0.86      0.88       995\n",
      "          9       0.89      0.89      0.89      1008\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n",
      "[[ 953    0    8    3    1   13   14    0    5    1]\n",
      " [   0 1104    3    3    0    3    0    3   11    0]\n",
      " [   9    4  871   11   11    7   13   18   21    3]\n",
      " [   3    7   35  939    1   28    1   13   20   12]\n",
      " [   3    8    4    1  854    2   11    0    4   27]\n",
      " [  11    6   10   31   10  790   18    5   23   10]\n",
      " [  10    4   10    0    7    7  907    0    6    0]\n",
      " [   3    6   11    2   10    2    0  984    1   47]\n",
      " [   7   33    9   23    3   36    4    6  858   16]\n",
      " [   5    6    7   12   33    8    2   33    6  896]]\n",
      "91.56\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "paramaters = [\n",
    "             {'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000,10000, 100000]}                                       \n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we bekomen een accuracy van 90% op de train data\n",
    "# de beste C is één van 100K dit is normaal gezien een zeer grote regularisatie maar bij logistic regression werkt dit omgekeerd\n",
    "# het model is dus niet sterk geregulariseerd (kans op underfitting)\n",
    "# maar dit is niet het geval want ook bij de testdata bekomen we een accuarcy van 91% \n",
    "#opmerking: dit duurte nog één lang om te trainen.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.9455\n",
      "Best parameters : {'C': 10, 'kernel': 'poly'}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.97       998\n",
      "          1       0.99      0.99      0.99      1127\n",
      "          2       0.95      0.93      0.94       968\n",
      "          3       0.97      0.94      0.95      1059\n",
      "          4       0.93      0.96      0.95       914\n",
      "          5       0.96      0.95      0.96       914\n",
      "          6       0.97      0.97      0.97       951\n",
      "          7       0.97      0.94      0.96      1066\n",
      "          8       0.90      0.96      0.93       995\n",
      "          9       0.93      0.92      0.93      1008\n",
      "\n",
      "avg / total       0.95      0.95      0.95     10000\n",
      "\n",
      "[[ 970    0    4    1    2    0   13    0    8    0]\n",
      " [   0 1115    3    1    2    1    0    0    4    1]\n",
      " [   6    0  899    4    8    2    4    5   39    1]\n",
      " [   0    2   13  997    2    6    0    5   27    7]\n",
      " [   0    2    4    0  880    2    4    1    2   19]\n",
      " [   3    0    2   16    5  872    2    1    9    4]\n",
      " [   5    2    6    0    7   10  918    0    3    0]\n",
      " [   2    1    6    1   11    0    0 1007    6   32]\n",
      " [   3    5    2    5    2   15    1    0  954    8]\n",
      " [   4    4    3    8   25    3    0   18   13  930]]\n",
      "95.42\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC()\n",
    "paramaters = [ \n",
    "        {'kernel': ['linear'], 'C': np.linspace(1,20,100)},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},\n",
    "        {'kernel': ['poly'], 'C':[1, 10]} ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We bekomen een accuracy van 94.5% op de traindata, 95.5 op testdata\n",
    "#de beste kernel is een polynomial (meerderegraads kernel) & de beste C parameter is 10,\n",
    "#hoe hoger de C smaller de kernel, hoe meer kans op overfitting (niet het geval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
